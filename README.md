# VERA

![status](https://img.shields.io/badge/status-WIP-yellow)
**This project is a work in progress. Nothing is finalized. Data is placeholder/made-up. Use at your own risk.**


# Values-Embedded Retrieval Architecture  
### A Transparent Framework for Ethical Pluralism in Organizational AI

**Jay William Frink**  
*May, 2025*


## Introduction

Artificial intelligence is reshaping how decisions are made: faster, broader, and with less human oversight. But behind every AI-generated recommendation or policy lies a set of implicit values, often invisible to the people they affect. The Values-Embedded Retrieval Architecture (VERA) framework makes those values transparent, traceable, and open to debate.

VERA allows organizations to encode multiple values frameworks - such as ethical codes, guiding principles, cultural norms, corporate policies, or belief systems - into an AI-accessible knowledge base. These values are then semantically retrieved and integrated into the AI’s reasoning process. Instead of hiding behind the illusion of “neutrality,” VERA empowers stakeholders to see, question, and shape the moral assumptions behind their AI-generated interactions.

VERA was conceived by Jay William Frink, a product manager and strategist working at the intersection of AI, governance, and organizational design. The idea emerged from watching a routine corporate training addressing the problem of bias in AI. The host - a prominent AI influencer and Silicon Valley technologist - innocently stated:

> “It’s important to be vigilant in removing our biases from training data sets so that we don’t inadvertently, for example, discourage women and young girls from rewarding opportunities. We could be depriving ourselves of the next CEO!”

If, during actual *“Removing Bias in AI”* training, it’s possible for an industry expert to unconsciously display such a wide range of his own biases in a single statement (i.e., that women and girls are conditionally worthy of being treated equally to the extent they produce future economic value; that becoming an executive is the penultimate personal achievement; that corporate careers are more worthy of recognition than those in education, healthcare, the arts, etc.), then we clearly must, as technologists, leverage our very best tools and thinking to intervene - meticulously, constructively, and transparently - by grounding our systems in explicitly stated values.

---

## The Technology

VERA is built on a Retrieval-Augmented Generation (RAG) architecture, but with a distinct purpose: to surface human values, not just facts. It connects a large language model to a searchable knowledge base containing an organization’s ethical codes, policies, guiding principles, or even philosophical texts - documents that define what the organization stands for.

These documents are split into smaller passages and stored in a vector database, which allows the AI to search by meaning rather than keywords. When asked a question about, for instance, organizational hiring practices or handling interpersonal disputes, VERA retrieves the most relevant value-aligned content and uses it to guide the AI’s response.

What makes VERA different is that it doesn’t default to one framework. It can present multiple value-grounded responses when trade-offs exist, making ethical tensions visible rather than burying them in a single output. A transparency layer tracks which values were used, what documents influenced the result, and what alternatives were available - providing a clear audit trail.

And what’s more, users can navigate to a web application that presents the curated values store that informs their AI responses in wiki form - reviewing, commenting, and proposing ideas in an egalitarian and highly-transparent setting.

Rather than hiding behind generalized training data, VERA grounds AI decisions in the organization’s own declared values, showing its work and inviting communities to join in a dialog to shape it.

---

## A New Standard of Transparency

Most AI systems obscure the logic behind their decisions. VERA does the opposite: exposing the values that guide its responses and the trade-offs it considers. This creates space not just for accountability, but for genuine review, reflection, and revision.

VERA offers built-in response accountability and auditability: every AI-generated output can be traced back to the specific value statements that shaped it. This gives organizations the framework they need to not only meet regulatory expectations, but to build a set of ethically robust, trustworthy tools for stakeholders. In short, to “do the right thing by doing the thing right.”

Crucially, VERA doesn’t enforce a singular worldview. It supports ethical pluralism, acknowledging that real organizations operate across diverse, often competing, cultural and normative contexts. In that light, transparency is not just a safeguard - it’s a mechanism for pluralistic governance.

By opening the black box, VERA transforms AI into a living document - one that reflects not just what we do, but what we believe.

